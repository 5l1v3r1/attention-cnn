
\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

% ADDED BY US
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{cleveref}

\usepackage{todonotes}
\setlength{\marginparwidth}{3.4cm}

\definecolor{red}{HTML}{e74c3c}
\definecolor{blue}{HTML}{3498db}
\definecolor{green}{HTML}{2ecc71}
% END ADDED BY US


\title{Formatting Instructions for ICLR 2020 \\ Conference Submissions}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
The abstract paragraph should be indented 1/2~inch (3~picas) on both left and
right-hand margins. Use 10~point type, with a vertical spacing of 11~points.
The word \textsc{Abstract} must be centered, in small caps, and in point size 12. Two
line spaces precede the abstract. The abstract must be limited to one
paragraph.
\end{abstract}

\section{Introduction}

Attention mechanism were introduced in Neural Machine Translation to better handle long range dependencies and align pairs of sentences \citep{Bahdanau2015attention}.

\section{Background on Attention Mechanism and Related Work}

We briefly recall the formulation of standard CNN layers and Self-Attention layers using unified notation.

\subsection{Convolution Layer}

A convolution layer is defined by its kernel size $K$ (assuming square kernel for simplicity), the number of input channel $C_{in}$ and the number of output channel $C_{out}$. 
It is parametrized by a kernel tensor $\tW~\in~\R^{K\times K \times C_{out} \times C_{in}}$ and a bias vector $\vb~\in~\R^{C_{out}}$.
Given an input image $\tX~\in~\R^{W\times H \times C}$ of width $W$, height $H$ and $C$ channels, 
the output of the convolution layer is given by,
%
\begin{align}
  \tY_{i,j,:} = 
  \vb 
  +
  \sum_{(m, n) \in \{\lfloor (K-1) / 2 \rfloor, \dots, \lfloor K / 2 \rfloor\}^2}
  \mW_{m,n,:,:}
  \tX_{i+m, j+n, :}
\end{align}
%


\subsection{Self-Attention Layer}

A Self-Attention layer is defined by its hidden dimension $d_{h}$ and its number of heads $K$. \todo{JB: same as size of the CNN kernel, we point that they are related ($K^2$) but need clarify notation} %
It is parametrized by
$\tW^Q \in \R^{K \times d \times d_{h}}$ query layer, $\tW^V \in \R^{K \times d \times d_{h}}$ key layer, $\tW^V \in \R^{K \times d \times d_{h}}$ value layer, a projection layer $\tW^Q \in \R^{K \times d_{h} \times d}$ and output layer $\mW^O \in \R^{(Kd_{h})\times d}$. 
%
Applied to an input $\mX \in \R^{T\times d}$, the output of the Self-Attention layer is computed as,
%
\begin{align}
  \mA_k &= \operatorname{softmax}
  \left(
    (\mX \mW_k^K)(\mX \mW_k^Q)^\top
  \right)\,, \\
  %
  \mH_{t,:} &= 
  \sum_{k \in {1,\dots, K}} 
  \mA_k
  \mX
  \mW_k^P
  \,,\\
  %
  \mY_{t,:} &= \mH_{t,:} \mW^O + \mX_{t,:}\,,
\end{align}
where the $\operatorname{softmax}(\cdot)$ is taken over all the dimensions of the tensor but the first one. For simplicity, we exclude the batch normalization layers.%
\todo{JB: check dimensions and readability}%


\subsection{Attention Mechanism in Vision}

Successful work to use attention on images \citep{ramachandran2019standaloneselfattention,belloAttentionAugmentedConvolutional2019}.


\begin{table}[h]
  \centering
  \begin{tabular}{lccc}
    \toprule
    Model&relative&sinusoids&learned\\
    \midrule
    \cite{vaswani17attentionisallyouneed} & & \checkmark\\
    \cite{radford2018gpt2} & & & \checkmark\\
    \cite{devlin2018bert} & & & \checkmark\\
    \cite{dai2019transformerxl} & \checkmark & \checkmark \\
    \cite{yang2019xlnet}  & \checkmark & \checkmark \\
    \midrule
    \cite{belloAttentionAugmentedConvolutional2019} & \checkmark & & \checkmark \\
    \cite{ramachandran2019standaloneselfattention} & \checkmark & & \checkmark \\
    % Images
    \bottomrule
  \end{tabular}
  \caption{Types of position encoding used by transformers models applied to text (\emph{top}) and images (\emph{bottom}). 
  When multiple encoding types have been tried, we report the one advised by the authors.}
  \label{tab:relwork_attention}
\end{table}


\section{Attention Layer Can Implement Convolution Layer}

Our goal is to show that attention layers have the expressive power to learn 
convolution filter and that we can learn such filters in practice.
%
The ability of the Attention mechanism to encode spacial filters (3$\times$3 kernels) 
depends heavily on how we encode position in the image.


Understanding how attention can apply convolutions can help to:
\begin{itemize}
  \item find better position encoding scheme,
  \item explain the need of multiple heads.
\end{itemize}

We take inspiration from \citep[TransformerXL]{dai2019transformerxl} to define relative positional encoding.
They decompose the computation of the attention coefficients as follow:
\begin{align}
    \mathbf{A}_{i, j}^{\mathrm{abs}} &= (\mE_{x_i} + \mU_i)^\top \mW_q^\top \mW_k (\mE_{x_j} + \mU_j)\\
  %
  %
 &=
  \underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}}_{(a)} 
  +
  \underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}}_{(b)}
  +
  \underbrace{\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}}_{(c)}
  +
  \underbrace{\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}}_{(d)}
\end{align}
with $\mE_{x_i}$ the embedding of token $x_i$, $\mU_i$ the absolute position encoding, $\mR_{i-j}$ relative position encoding. They replace all absolute position encoding with relative ones:
\begin{align}
  \mathbf{A}_{i, j}^{\mathrm{rel}} &=
  \underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, E} \mathbf{E}_{x_{j}}}_{(a)}
  +
  \underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, R} {\color{blue} \mathbf{R}_{i-j}}}_{(b)}
  +
  \underbrace{{\color{red} \vu^{\top}} \mathbf{W}_{k, E} \mathbf{E}_{x_{j}}}_{(c)}
  +
  \underbrace{{\color{red} \vv^{\top}} \mathbf{W}_{k, R} {\color{blue} \mathbf{R}_{i-j}}}_{(d)}
  \label{eq:att_rel}
\end{align}

To mimic the CNN computation, each attention head focuses on a pixel at a given relative position.
Hence reproducing a $3\times 3$ kernel requires 9 attention heads and the ability for each of them to perfectly attend on
one pixel at a relative position.
Given that spatial convolution filters are not conditioned on the input data, we set $\mW_{k, E}$ and $\mW_{k, R}$ to 0, leaving only the (d) term in \eqref{eq:att_rel}.
We further write the attention computation in the 2D setting,
\begin{align}
  \mathbf{A}_{i, j}^{\mathrm{conv}} &= \vv^{\top} \mathbf{W}_{k, R} {\color{blue} \mathbf{R}_{\vp_i - \vp_j}}
\end{align}
with $\vp_i$ the row and column position of pixel $i$. As both $\vv$ and $\mathbf{W}_{k, R}$ are learnable parameters, we simplify the expression to learn a single vector $\mathbf{t}_h$ for each head $h$.

\subsection{Attention on a Pixel at any Relative Position}

\paragraph{How to discern relative position?} The goal is to reproduce the $3\times 3$ pattern of CNN kernel with attention instead.
Each head should be able to chose which pixel to attend based on its relative position to the query token $x_i$.
The most basic attention mechanism is the dot product $(\vp_i - \vp_j)^\top \mathbf{t}_h$ between the relative positions of the input pixels and the relative target positions $\mathbf{t}_h = (x_h, y_h)^\top$.
The dot product is large for pixels in the same direction as $\mathbf{t}_h$, however it also grows for farther pixels. To be able to focus on a relative pixel position (direction and distance), we propose the following relative position encoding
\begin{align}
  \mR_{\Delta} = 
  \begin{pmatrix} 
  \Delta_x& \Delta_y& \Delta_x^2& \Delta_y^2& 1& 1
  \end{pmatrix}^\top
\end{align}
Instead of learning the target vector $\mathbf{t}_h$, we parametrize it for each head to attend to the relative pixel position~$(x_h, y_h)$,
\begin{align}
  \mathbf{t}_h= 
  -\alpha_h \begin{pmatrix} 
  -2 x_h& -2 y_h& 1& 1&  x_h^2&  y_h^2
  \end{pmatrix}^\top
\end{align}
The attention coefficient for head $h$ between pixel $i$ and $j$ is given by the dot product,
\begin{align}
  \mR_{\Delta}^\top \mathbf{t}_h = -\alpha_h ( (\Delta_x - x_h)^2 + (\Delta_y - y_h) ^2 ),
\end{align}
with $\Delta = \vp_i - \vp_j$. 
The maximum attention coefficient is 0 when $\Delta = (x_h, y_h)$, i.e. the center of attention of a given head.
The $\alpha$ coefficient controls how spiky the attention is (analogous to temperature in the softmax).
\Cref{fig:attention_cnn} gives a representation of the attention weights for a fixed query pixel $i$ and different $\alpha$ and $(x_h, y_h)$.

\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{plots/conv_attention.pdf}
  \caption{Attention coefficient for different center $i$ and target $j$ with varying $\alpha$ parameters. Target designate the relative position that the head attends to.}
  \label{fig:attention_cnn}
\end{figure}

\paragraph{Discussion.} Some relations can be drawn to RBF kernels centered at the target pixels.
We are expressing a more general form of convolutions where
\emph{(i)} the input pixels do not follow a grid shape but has any pattern and 
\emph{(ii)} the inputs are not single pixels but weighted averages of local patches in the image. 

\paragraph{Simplification.} One further simplification is to remove the constant terms of $\mR_{\Delta}$ because the softmax (renormalization) is shift invariant and constant terms for a head are useless. 


\subsection{Non Isotropic Gaussian}

In this section, we present an extension of the relative positional encoding introduced above
to model non-isotropic Gaussians.

\begin{align}
  f_{\vmu, \mSigma}(\vx) = \exp \left(
    - \frac 1 2
    (\vx - \vmu)^\top \mSigma^{-1} (\vx - \vmu)
  \right)
\end{align}
We drop the normalization factor of the Gaussian distribution as it is replaced by the softmax of the attention mechanism.
The matrix $\mSigma^{-1}$ must be positive semi-definite.
We parametrize it as 
\begin{align}
  \mSigma^{-1} = (\mSigma^{-1/2})^\top\mSigma^{-1/2} = \begin{pmatrix}a & b \\ b & c\end{pmatrix}  
\end{align}
Our goal is to write the exponent as a dot product of two vectors:
\begin{itemize}
  \item one vector $R_\vx$ dependent only on $\vx$, the relative positional encoding,
  \item one vector $\mathbf{t}_h$ dependent only on $\vmu$ and $\mSigma$, the attention head.
\end{itemize}
\begin{align}
  R_\vx &= 
  \begin{pmatrix}
    x_1      & x_2       & x_1      & x_2      & x_1^2   & x_2^2 & x_1 x_2 & 1        & 1         & 1
  \end{pmatrix}\\
  \mathbf{t}_h &= -\frac{1}{2}
  \begin{pmatrix}
    -2a\mu_1 & -2c \mu_2 & -2b\mu_2 & -2b\mu_1 & a       & c     & 2b      & a\mu_1^2 & c\mu_2^2 & 2b\mu_1 \mu_2
  \end{pmatrix}\\
  R_\vx^\top \mathbf{t}_h &=
  -\frac{1}{2} 
  \left[
  a(x_1 - \mu_1)^2 + 2b(x_1-\mu_1)(x_2 -\mu_2) + c(x_2-\mu_2)^2 
  \right]
  = - \frac 1 2 (\vx - \vmu)^\top \mSigma^{-1} (\vx - \vmu)
\end{align}
We can check that setting $b = 0$ and $a=c=\alpha$ recovers the expression of the isotropic Gaussian derived above.

\subsection{Implications to Sinusoid or Learned Positional Encoding} 


\subsection{Exploit Attention on Features?} 
\begin{itemize}

  \item Does the terms (a) (b) (c) allow to condition the CNN filters on the input data?
\end{itemize}


\section{Experiments}

We want to validate that not only Transformer architecture can express CNN filters
but that it can also learn such filters with SGD from the data.
%
It has already been shown by \citep{belloAttentionAugmentedConvolutional2019} that 
adding attention features to CNNs can improve performence on Cifar-100 and ImageNet.
%
We want to stick to a fully attentional model, and show that it matches its siblings ResNet of equivalent depth.

\subsection{Performance Against ResNet}

We design a smaller ResNet (i.e. ResNet10) which have similar number of layers as our transformer.
The sizes of the models are displayed in Table~\ref{tab:parameter_size}. 

\begin{table}
  \centering
  \begin{tabular}{cc}
    \toprule
    Model & Number of parameters\\
    \midrule
    ResNet10 & 4.9M\\
    BERT 4 layers & 1.85M\\
    BERT 6 layers & 9.56M \\
    \bottomrule
  \end{tabular}
  \caption{Number of parameters per model.}
  \label{tab:parameter_size}
\end{table}

\begin{figure}[h]
\includegraphics[width=.16\linewidth]{plots/attention/layer_0.png}
\includegraphics[width=.16\linewidth]{plots/attention/layer_1.png}
\includegraphics[width=.16\linewidth]{plots/attention/layer_2.png}
\includegraphics[width=.16\linewidth]{plots/attention/layer_3.png}
\includegraphics[width=.16\linewidth]{plots/attention/layer_4.png}
\includegraphics[width=.16\linewidth]{plots/attention/layer_5.png}
\caption{Contours of attention weights per layer for the 6 layers fully attentional model.}
\end{figure}

\begin{figure}[h]
\includegraphics[width=.13\linewidth]{plots/attention_training/layer_3_0.png}
\includegraphics[width=.13\linewidth]{plots/attention_training/layer_3_1.png}
\includegraphics[width=.13\linewidth]{plots/attention_training/layer_3_2.png}
\includegraphics[width=.13\linewidth]{plots/attention_training/layer_3_3.png}
\includegraphics[width=.13\linewidth]{plots/attention_training/layer_3_4.png}
\includegraphics[width=.13\linewidth]{plots/attention_training/layer_3_5.png}
\includegraphics[width=.13\linewidth]{plots/attention_training/layer_3_6.png}
\caption{Contours of attention weights during training (300 epochs) at layer 3.}
\end{figure}

\begin{figure}[h]
  \includegraphics[width=.4\linewidth]{plots/eval_accuracy_plot.png}
  \hfill
  \includegraphics[width=.6\linewidth]{plots/eval_accuracy_legend.png}
  \caption{Evaluation accuracy on CIFAR-10 of a small ResNet and two Gaussian Attention models.}
\end{figure}



\section{Discussion}



% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}
Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{bibliography}
\bibliographystyle{iclr2020_conference}

% \appendix
% \section{Appendix}
% You may include other additional sections here. 

\end{document}
