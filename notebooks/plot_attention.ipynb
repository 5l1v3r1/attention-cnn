{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import os\n",
    "import train\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"output/gaussian_cnn_4l_16h_alpha_pos_init\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(open(os.path.join(directory, \"config.yaml\"), \"r\"))\n",
    "train.config = config\n",
    "model = get_model('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_data = torch.load(os.path.join(directory, \"best.checkpoint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint_data['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_per_layer = [layer.attention.self.attention_alpha.detach().cpu() for layer in model.encoder.layer]\n",
    "centers_per_layer = [layer.attention.self.attention_centers.detach().cpu() for layer in model.encoder.layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    # softmax over all dim but last one\n",
    "    max_per_head = X.view(-1, X.shape[-1]).max(dim=0)[0]\n",
    "    X -= max_per_head\n",
    "    exp_X = X.exp()\n",
    "    normalizer = exp_X.view(-1, X.shape[-1]).sum(dim=0)\n",
    "    Y = exp_X / normalizer.view(1,1,-1)\n",
    "    return Y\n",
    "\n",
    "MAX_WIDTH_HEIGHT = 50\n",
    "range_ = torch.arange(MAX_WIDTH_HEIGHT)\n",
    "grid = torch.cat([t.unsqueeze(-1) for t in torch.meshgrid([range_, range_])], dim=-1)\n",
    "relative_indices = grid.unsqueeze(0).unsqueeze(0) - grid.unsqueeze(-2).unsqueeze(-2)\n",
    "R = torch.cat([relative_indices, relative_indices ** 2, torch.ones_like(relative_indices)], dim=-1)\n",
    "R = R.float()\n",
    "\n",
    "def plot_attention_positions(relative_positions, alphas, width=20, ax=None):\n",
    "    relative_encoding_from_center = R[width // 2, width // 2, :width, :width,]\n",
    "    targets = torch.cat([-2 * relative_positions, torch.ones_like(relative_positions), relative_positions ** 2], dim=-1)\n",
    "    \n",
    "    attention_scores = torch.einsum('ijd,hd->ijh', [relative_encoding_from_center, targets])\n",
    "    attention_scores /= - alphas.view(1,1,-1) # rescaling\n",
    "    attention_probs = softmax(attention_scores)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    n_head = len(alphas)\n",
    "    for i in range(n_head):\n",
    "        cs = ax.contour(attention_probs[:,:,i], levels=[0.1, 0.4], colors=f\"C{i}\")\n",
    "        \n",
    "    ax.set_xticks([], [])\n",
    "    ax.set_yticks([], [])\n",
    "    ax.set_aspect(aspect=1)\n",
    "    ax.scatter([width//2], [width//2], c='r', zorder=2)\n",
    "    \n",
    "# plot_attention_positions(gaussian_shifts, gaussian_alpha, width=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(layers), figsize=(24,6))\n",
    "\n",
    "for i in range(len(layers)):\n",
    "    plot_attention_positions(centers_per_layer[i], alpha_per_layer[i].exp(), width=32, ax=axes[i])\n",
    "    axes[i].set_title(f\"Layer {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
