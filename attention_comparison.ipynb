{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "\n",
    "MAX_WIDTH_HEIGHT = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilat(tensor, dilations):\n",
    "    \"\"\"\n",
    "    tensor  (tensor): Tensor to be dilated\n",
    "    dilations (list): List of dilation factor for each dim,\n",
    "                      None for no dilation on this dim\n",
    "\n",
    "    For each dilated dim of size s, it splits this dim into to (s // dil, dil) dimensions\n",
    "    Otherwise, does nothing to the dim\n",
    "    \"\"\"\n",
    "    assert len(tensor.shape) == len(dilations)\n",
    "    assert all(\n",
    "        (dil is None) or (sh % dil == 0) for dil, sh in zip(dilations, tensor.shape)\n",
    "    ), \"dilation should divide dimension\"\n",
    "\n",
    "    new_dim = []\n",
    "    content_indices = []\n",
    "    group_indices = []\n",
    "    for dim, dilat in zip(tensor.shape, dilations):\n",
    "        if dilat is None:\n",
    "            new_dim.append(dim)\n",
    "        else:\n",
    "            new_dim.extend([dim // dilat, dilat])\n",
    "\n",
    "    tensor_dilat = tensor.view(*new_dim)\n",
    "    return tensor_dilat\n",
    "\n",
    "\n",
    "def dilated_attention(V, Q, K, dilation=1):\n",
    "    try:\n",
    "        x_dilation, y_dilation = dilation\n",
    "    except:\n",
    "        x_dilation = y_dilation = dilation\n",
    "\n",
    "    # V shape: B x W x H x n_head x d_v\n",
    "    # Q shape: B x W x H x n_head x d_k\n",
    "    # K shape: B x W x H x n_head x d_k\n",
    "    batch_size, width, height, n_head, d_v = V.shape\n",
    "    d_k = Q.shape[-1]\n",
    "\n",
    "    # B x W/dil x dil x H/dil x dil x n_head x d\n",
    "    K_dilated = dilat(K, [None, x_dilation, y_dilation, None, None])\n",
    "    Q_dilated = dilat(Q, [None, x_dilation, y_dilation, None, None])\n",
    "    V_dilated = dilat(V, [None, x_dilation, y_dilation, None, None])\n",
    "\n",
    "    # b = batch, h = head, d = dimension\n",
    "    # each pixel can attend pixels in its own group,\n",
    "    # i.e. all position at a distance multiple of (x_dil, y_dil)\n",
    "    # blocks are (x_dil X y_dil) rectangle of the image\n",
    "    # each pixel of block (x,y) in the group (i,j) attend pixel in same group in different blocks (v,w)\n",
    "    # a dot product is done over the d dimension\n",
    "    # the head and batch dimension are kept\n",
    "    attention_coefficients = torch.einsum(\"bxiyjhd,bviwjhd->bxiyjhvw\", [Q_dilated, K_dilated])\n",
    "    #attention_coefficients = torch.einsum(\"bxiyjhd,bviwjhd->bxiyjvwh\", [Q_dilated, K_dilated])\n",
    "    attention_shape = attention_coefficients.size()\n",
    "    attention_coefficients = attention_coefficients.view(attention_shape[:-2] + (-1,))\n",
    "    attention_probs = nn.Softmax(dim=-1)(attention_coefficients)\n",
    "    attention_coefficients = attention_probs.view(attention_shape)\n",
    "\n",
    "\n",
    "\n",
    "    # the attention_coefficients are used to compute the weighted sum of the values\n",
    "    # each pixel in block (x,y) and group (i,j) sums the values of\n",
    "    # the pixes in group (i,j) at any other block position (v,w)\n",
    "    new_V = torch.einsum(\"bxiyjhvw,bviwjhd->bxiyjhd\", [attention_coefficients, V_dilated])\n",
    "    new_V = new_V.contiguous().view(batch_size, width, height, n_head*d_v)\n",
    "    #print(new_V.shape)\n",
    "    return new_V, attention_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unfolded(tensor,kernel_size):\n",
    "    \"\"\"\n",
    "    tensor  (tensor): Tensor to be dilated, in shape (batch_size, W, H, nhead*d)\n",
    "    kernel_size: size of the square to be attended\n",
    "\n",
    "    output:\n",
    "        tensor_unf: results in shape (batch, nhead*d, W, H, kernel_size, kernel_size)\n",
    "\n",
    "    \"\"\"\n",
    "    B, W, H, D = tensor.shape\n",
    "    tensor = tensor.permute(0,3,1,2)\n",
    "    unf = nn.Unfold(kernel_size=kernel_size, dilation=1, padding=int((kernel_size-1)/2), stride=1)\n",
    "    tensor_unf = unf(tensor)\n",
    "    tensor_unf = tensor_unf.view((B, D, W, H , kernel_size, kernel_size))\n",
    "    return tensor_unf\n",
    "\n",
    "\n",
    "def local_attention(V, Q, K, kernel_size=5):\n",
    "\n",
    "    # V shape: B x W x H x n_head x d_v\n",
    "    # Q shape: B x W x H x n_head x d_k\n",
    "    # K shape: B x W x H x n_head x d_k\n",
    "    batch_size, width, height, n_head, d_v = V.shape\n",
    "    V = V.view((batch_size, width, height, -1))\n",
    "    #Q = Q.view((batch_size, width, height, -1))\n",
    "    K = K.view((batch_size, width, height, -1))\n",
    "\n",
    "    d_k = Q.shape[-1]\n",
    "\n",
    "    K_field = get_unfolded(K,kernel_size).view((batch_size, n_head, d_v, width, height, kernel_size, kernel_size))\n",
    "    V_field = get_unfolded(V,kernel_size).view((batch_size, n_head, d_v, width, height, kernel_size, kernel_size))\n",
    "    #Q_field = get_unfolded(Q)\n",
    "\n",
    "    # b = batch, h = head, d = dimension\n",
    "    # each pixel can attend pixels in its own group,\n",
    "    # i.e. all position at a distance multiple of (x_dil, y_dil)\n",
    "    # blocks are (x_dil X y_dil) rectangle of the image\n",
    "    # each pixel of block (x,y) in the group (i,j) attend pixel in same group in different blocks (v,w)\n",
    "    # a dot product is done over the d dimension\n",
    "    # the head and batch dimension are kept\n",
    "    attention_coefficients = torch.einsum(\"bwhnd,bndwhxy->bwhnxy\", [Q, K_field])\n",
    "    #attention_coefficients = torch.einsum(\"bxiyjhd,bviwjhd->bxiyjvwh\", [Q_dilated, K_dilated])\n",
    "    attention_shape = attention_coefficients.size()\n",
    "    attention_coefficients = attention_coefficients.view(attention_shape[:-2] + (-1,))\n",
    "    attention_probs = nn.Softmax(dim=-1)(attention_coefficients)\n",
    "    attention_coefficients = attention_probs.view(attention_shape)\n",
    "\n",
    "\n",
    "\n",
    "    # the attention_coefficients are used to compute the weighted sum of the values\n",
    "    # each pixel in block (x,y) and group (i,j) sums the values of\n",
    "    # the pixes in group (i,j) at any other block position (v,w)\n",
    "    new_V = torch.einsum(\"bwhnxy,bndwhxy->bwhnd\", [attention_coefficients, V_field])\n",
    "    #print(new_V.shape)\n",
    "    new_V = new_V.contiguous().view(batch_size, width, height, n_head* d_v)\n",
    "    return new_V, attention_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttentionDilation(nn.Module):\n",
    "    def __init__(self, config, output_attentions=False, keep_multihead_output=False, dilations=None, kernel_size=5):\n",
    "        super(BertSelfAttentionDilation, self).__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
    "        self.output_attentions = output_attentions\n",
    "        self.keep_multihead_output = keep_multihead_output\n",
    "        self.multihead_output = None\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        self.dilations = dilations\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.proj = nn.Linear(4*config.hidden_size, config.hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, head_mask=None):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        if self.dilations is not None:\n",
    "            q_shape = mixed_query_layer.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "            query_layer = mixed_query_layer.view(*q_shape)\n",
    "            key_layer = mixed_key_layer.view(*q_shape)\n",
    "            value_layer = mixed_value_layer.view(*q_shape)\n",
    "            context_layer_dil, attention_probs = dilated_attention(value_layer, query_layer, key_layer, dilation=self.dilations)\n",
    "            context_layer_row, attention_probs_row = dilated_attention(value_layer, query_layer, key_layer,\n",
    "                                                                       dilation=(1, value_layer.shape[2]))\n",
    "            context_layer_col,attention_probs_col = dilated_attention(value_layer, query_layer, key_layer,\n",
    "                                                                       dilation=(value_layer.shape[1],1))\n",
    "            context_layer_local,attention_probs_local = local_attention(value_layer, query_layer, key_layer,\n",
    "                                                                       self.kernel_size)\n",
    "            #print(context_layer_dil.shape, context_layer_local.shape, context_layer_row.shape)\n",
    "            context_layer_cat = torch.cat((context_layer_dil,context_layer_row,context_layer_col,context_layer_local),\n",
    "                                          dim=-1)\n",
    "            context_layer = self.proj(context_layer_cat)\n",
    "\n",
    "            #TODO: Linear projection or weighted sum?\n",
    "        else:\n",
    "            query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "            key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "            value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "            # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "            attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "            attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "            # Normalize the attention scores to probabilities.\n",
    "            attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "            # This is actually dropping out entire tokens to attend to, which might\n",
    "            # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "            attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "            # Mask heads if we want to\n",
    "            if head_mask is not None:\n",
    "                attention_probs = attention_probs * head_mask\n",
    "\n",
    "            context_layer = torch.matmul(attention_probs, value_layer)\n",
    "            if self.keep_multihead_output:\n",
    "                self.multihead_output = context_layer\n",
    "                self.multihead_output.retain_grad()\n",
    "\n",
    "            context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "            new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "            context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        if self.output_attentions:\n",
    "            return attention_probs, context_layer\n",
    "        return context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    dataset=\"Cifar10\",\n",
    "    model=\"bert\",\n",
    "    optimizer=\"SGD\",\n",
    "    optimizer_decay_at_epochs=[150, 250],\n",
    "    optimizer_decay_with_factor=10.0,\n",
    "    optimizer_learning_rate=0.1,\n",
    "    optimizer_momentum=0.9,\n",
    "    optimizer_weight_decay=0.0001,\n",
    "    batch_size=16,\n",
    "    num_epochs=300,\n",
    "    seed=42,\n",
    "    # added for BERT, some are useless\n",
    "    vocab_size_or_config_json_file=-1,\n",
    "    hidden_size=128,  # 768,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=4,\n",
    "    intermediate_size=512,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=512,\n",
    "    type_vocab_size=2,\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-12,\n",
    "    # BERT Image specific\n",
    "    mask_dimension=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul  8 17:28:47 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 375.51                 Driver Version: 375.51                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K40c          Off  | 0000:04:00.0     Off |                    0 |\n",
      "| 25%   49C    P0    66W / 235W |      0MiB / 11439MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K40c          Off  | 0000:84:00.0     Off |                    0 |\n",
      "| 23%   38C    P0    68W / 235W |      0MiB / 11439MiB |     98%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID  Type  Process name                               Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig(object):\n",
    "    \"\"\"Configuration class to store the configuration of a `BertModel`.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 vocab_size_or_config_json_file,\n",
    "                 hidden_size=768,\n",
    "                 num_hidden_layers=12,\n",
    "                 num_attention_heads=12,\n",
    "                 intermediate_size=3072,\n",
    "                 hidden_act=\"gelu\",\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 attention_probs_dropout_prob=0.1,\n",
    "                 max_position_embeddings=512,\n",
    "                 type_vocab_size=2,\n",
    "                 initializer_range=0.02,\n",
    "                 layer_norm_eps=1e-12):\n",
    "        \"\"\"Constructs BertConfig.\n",
    "\n",
    "        Args:\n",
    "            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n",
    "            hidden_size: Size of the encoder layers and the pooler layer.\n",
    "            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n",
    "            num_attention_heads: Number of attention heads for each attention layer in\n",
    "                the Transformer encoder.\n",
    "            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
    "                layer in the Transformer encoder.\n",
    "            hidden_act: The non-linear activation function (function or string) in the\n",
    "                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n",
    "            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
    "                layers in the embeddings, encoder, and pooler.\n",
    "            attention_probs_dropout_prob: The dropout ratio for the attention\n",
    "                probabilities.\n",
    "            max_position_embeddings: The maximum sequence length that this model might\n",
    "                ever be used with. Typically set this to something large just in case\n",
    "                (e.g., 512 or 1024 or 2048).\n",
    "            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
    "                `BertModel`.\n",
    "            initializer_range: The sttdev of the truncated_normal_initializer for\n",
    "                initializing all weight matrices.\n",
    "            layer_norm_eps: The epsilon used by LayerNorm.\n",
    "        \"\"\"\n",
    "        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n",
    "                        and isinstance(vocab_size_or_config_json_file, unicode)):\n",
    "            with open(vocab_size_or_config_json_file, \"r\", encoding='utf-8') as reader:\n",
    "                json_config = json.loads(reader.read())\n",
    "            for key, value in json_config.items():\n",
    "                self.__dict__[key] = value\n",
    "        elif isinstance(vocab_size_or_config_json_file, int):\n",
    "            self.vocab_size = vocab_size_or_config_json_file\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_hidden_layers = num_hidden_layers\n",
    "            self.num_attention_heads = num_attention_heads\n",
    "            self.hidden_act = hidden_act\n",
    "            self.intermediate_size = intermediate_size\n",
    "            self.hidden_dropout_prob = hidden_dropout_prob\n",
    "            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "            self.max_position_embeddings = max_position_embeddings\n",
    "            self.type_vocab_size = type_vocab_size\n",
    "            self.initializer_range = initializer_range\n",
    "            self.layer_norm_eps = layer_norm_eps\n",
    "        else:\n",
    "            raise ValueError(\"First argument must be either a vocabulary size (int)\"\n",
    "                             \"or the path to a pretrained model config file (str)\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, json_object):\n",
    "        \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n",
    "        config = BertConfig(vocab_size_or_config_json_file=-1)\n",
    "        for key, value in json_object.items():\n",
    "            config.__dict__[key] = value\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_json_file(cls, json_file):\n",
    "        \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
    "        with open(json_file, \"r\", encoding='utf-8') as reader:\n",
    "            text = reader.read()\n",
    "        return cls.from_dict(json.loads(text))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
    "\n",
    "    def to_json_file(self, json_file_path):\n",
    "        \"\"\" Save this instance to a json file.\"\"\"\n",
    "        with open(json_file_path, \"w\", encoding='utf-8') as writer:\n",
    "            writer.write(self.to_json_string())\n",
    "\n",
    "try:\n",
    "    from apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm\n",
    "except ImportError:\n",
    "    #logger.info(\"Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\")\n",
    "    class BertLayerNorm(nn.Module):\n",
    "        def __init__(self, hidden_size, eps=1e-12):\n",
    "            \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
    "            \"\"\"\n",
    "            super(BertLayerNorm, self).__init__()\n",
    "            self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "            self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "            self.variance_epsilon = eps\n",
    "\n",
    "        def forward(self, x):\n",
    "            u = x.mean(-1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "            return self.weight * x + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "bert_config = BertConfig.from_dict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "attndil = BertSelfAttentionDilation(config=bert_config,dilations=4)\n",
    "attndil = attndil.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'Cifar10',\n",
       " 'model': 'bert',\n",
       " 'optimizer': 'SGD',\n",
       " 'optimizer_decay_at_epochs': [150, 250],\n",
       " 'optimizer_decay_with_factor': 10.0,\n",
       " 'optimizer_learning_rate': 0.1,\n",
       " 'optimizer_momentum': 0.9,\n",
       " 'optimizer_weight_decay': 0.0001,\n",
       " 'batch_size': 16,\n",
       " 'num_epochs': 300,\n",
       " 'seed': 42,\n",
       " 'vocab_size_or_config_json_file': -1,\n",
       " 'hidden_size': 128,\n",
       " 'num_hidden_layers': 4,\n",
       " 'num_attention_heads': 4,\n",
       " 'intermediate_size': 512,\n",
       " 'hidden_act': 'gelu',\n",
       " 'hidden_dropout_prob': 0.1,\n",
       " 'attention_probs_dropout_prob': 0.1,\n",
       " 'max_position_embeddings': 512,\n",
       " 'type_vocab_size': 2,\n",
       " 'initializer_range': 0.02,\n",
       " 'layer_norm_eps': 1e-12,\n",
       " 'mask_dimension': 5}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config, output_attentions=False, keep_multihead_output=False):\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
    "        self.output_attentions = output_attentions\n",
    "        self.keep_multihead_output = keep_multihead_output\n",
    "        self.multihead_output = None\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, head_mask=None):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "        \n",
    "        #print(query_layer.shape)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        \n",
    "        #print(attention_scores.shape)\n",
    "        \n",
    "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        if self.keep_multihead_output:\n",
    "            self.multihead_output = context_layer\n",
    "            self.multihead_output.retain_grad()\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        if self.output_attentions:\n",
    "            return attention_probs, context_layer\n",
    "        return context_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "attnos = []\n",
    "for i in range(12):\n",
    "    attnos.append(BertSelfAttention(config=bert_config).to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attnos = []\n",
    "for i in range(12):\n",
    "    attnos.append(BertSelfAttentionDilation(config=bert_config, dilations=4).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 14\n",
      "width 32\n",
      "height 32\n",
      "n_head 16\n",
      "d 128\n",
      "dil 4\n"
     ]
    }
   ],
   "source": [
    "batch_size = 14\n",
    "width = 32\n",
    "height = 32\n",
    "n_head = 16\n",
    "d = 128\n",
    "dil = 4\n",
    "\n",
    "print(\"batch_size\", batch_size)\n",
    "print(\"width\", width)\n",
    "print(\"height\", height)\n",
    "print(\"n_head\", n_head)\n",
    "print(\"d\", d)\n",
    "print(\"dil\", dil)\n",
    "\n",
    "#batch = torch.rand(batch_size, width, height, d)\n",
    "#batch_flat = torch.rand(batch_size, width*height, d).to(device)\n",
    "#attention_mask = torch.ones(batch_size, width, height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask_flat = torch.ones(width*height, width*height).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch.to(device)\n",
    "attention_mask = attention_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB (GPU 0; 11.17 GiB total capacity; 10.79 GiB already allocated; 115.75 MiB free; 479.00 KiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9e6020fe462a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbatch_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mattnorig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattnos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mcontext_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattnorig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-1ee2dbebaf19>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m#print(attention_scores.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 0; 11.17 GiB total capacity; 10.79 GiB already allocated; 115.75 MiB free; 479.00 KiB cached)"
     ]
    }
   ],
   "source": [
    "num_steps = 20\n",
    "\n",
    "for i in tqdm(range(num_steps)):\n",
    "    context_layers = []\n",
    "    attention_mask_flat = torch.ones(width*height, width*height).to(device)\n",
    "    batch_flat = torch.rand(batch_size, width*height, d).to(device)\n",
    "    for attnorig in attnos:\n",
    "        context_layers.append(attnorig(batch_flat, attention_mask_flat))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:30<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20\n",
    "\n",
    "for i in tqdm(range(num_steps)):\n",
    "    context_layers = []\n",
    "    batch = torch.rand(batch_size, width, height, d).to(device)\n",
    "    attention_mask = torch.ones(batch_size, width, height).to(device)\n",
    "    for attnorig in attnos:\n",
    "        context_layers.append(attnorig(batch, attention_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
